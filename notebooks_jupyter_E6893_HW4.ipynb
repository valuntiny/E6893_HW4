{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guojing Wu** | UNI: gw2383 | *2019-11-14*\n",
    "\n",
    "# E6893 Big Data Analytics Homework4\n",
    "\n",
    "## This is for question 3\n",
    "\n",
    "Get the graphic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *  # for graph analysis\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "def getData(sc, filename):\n",
    "    \"\"\"\n",
    "    Load data from raw text file into RDD and transform.\n",
    "    Hint: transfromation you will use: map(<lambda function>).\n",
    "    Args:\n",
    "        sc (SparkContext): spark context.\n",
    "        filename (string): hw2.txt cloud storage URI.\n",
    "    Returns:\n",
    "        RDD: RDD list of tuple of (<User>, [friend1, friend2, ... ]),\n",
    "        each user and a list of user's friends\n",
    "    \"\"\"\n",
    "    # read text file into RDD\n",
    "    data = sc.textFile(filename).map(lambda line: line.split('\\t'))\n",
    "\n",
    "    # TODO: implement your logic here\n",
    "    data = data.map(lambda tmp: (tmp[0], [num for num in tmp[1].split(',')]))\n",
    "\n",
    "    return data\n",
    "\n",
    "def getEdges(line):\n",
    "    \"\"\"\n",
    "    get edges from input data\n",
    "    \n",
    "    Args:\n",
    "        line (tuple): a tuple of (<User1>, [(<User2>, 0), (<User3>, 1)....])\n",
    "    Returns:\n",
    "        RDD of tuple (line[0], connected friend)\n",
    "    \"\"\"\n",
    "    friends = line[1]\n",
    "    for i in range(len(friends)):\n",
    "        # Direct friend\n",
    "        yield (line[0], friends[i])\n",
    "        \n",
    "conf = SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "sc.setCheckpointDir('/checkpoints')\n",
    "# The directory for the file\n",
    "filename = \"gs://big_data_hw/hw2/q1.txt\"\n",
    "\n",
    "# Get data in proper format\n",
    "data = getData(sc, filename)\n",
    "vertices = data.map(lambda x: (x[0],))\n",
    "edges = data.flatMap(getEdges)\n",
    "V = spark.createDataFrame(vertices, [\"id\"])\n",
    "E = spark.createDataFrame(edges, [\"src\", \"dst\"])\n",
    "G = GraphFrame(V, E)\n",
    "compo = G.connectedComponents()\n",
    "\n",
    "# nodes\n",
    "qNodes = compo.filter(compo[\"component\"] == 103079215141).select('id')\n",
    "qNodes.write.save('gs://big_data_hw/hw4/ndoes', format=\"json\", mode=\"overwrite\")\n",
    "\n",
    "# edges \n",
    "tmp = [int(q.id) for q in qNodes.collect()]\n",
    "# filter query text\n",
    "qtext = \"(\"\n",
    "for i in range(len(tmp)-1):\n",
    "    qtext += \"src = {} or \".format(tmp[i])\n",
    "qtext += \"src = {}) and (\".format(tmp[-1])\n",
    "for i in range(len(tmp)-1):\n",
    "    qtext += \"dst = {} or \".format(tmp[i])\n",
    "qtext += \"dst = {})\".format(tmp[-1])\n",
    "\n",
    "qEdges = G.filterEdges(qtext).edges\n",
    "x = qEdges.toPandas().replace([str(i) for i in tmp], [str(i) for i in range(len(tmp))])\n",
    "qEdges = spark.createDataFrame(x)\n",
    "qEdges.write.save('gs://big_data_hw/hw4/edges', format=\"json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# nodes\n",
    "files = 'gs://big_data_hw/hw4/ndoes' + '/part-*'\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files}'.format(\n",
    "        dataset='my_dataset', table='nodes', files=files\n",
    "    ).split())\n",
    "output_path = sc._jvm.org.apache.hadoop.fs.Path('gs://big_data_hw/hw4/ndoes')\n",
    "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "    output_path, True)\n",
    "\n",
    "# edges\n",
    "files = 'gs://big_data_hw/hw4/edges' + '/part-*'\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files}'.format(\n",
    "        dataset='my_dataset', table='edges', files=files\n",
    "    ).split())\n",
    "output_path = sc._jvm.org.apache.hadoop.fs.Path('gs://big_data_hw/hw4/edges')\n",
    "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "    output_path, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
